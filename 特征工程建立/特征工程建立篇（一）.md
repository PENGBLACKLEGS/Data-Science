# 特征工程建立篇（一）------特征选择

​        在许多大数据挖掘竞赛中（国内的阿里天池和国外的kaggle平台），最复杂的过程莫过于特征工程建立阶段，大概占据了整个竞赛过程的70%的时间和精力，最终建立的模型的好坏大多也取决于特征工程建立的好坏。

​        特征工程部分往往也是数据分析的核心，特征工程的建立往往比模型的建立复杂的多，特征工程不像模型建立的过程有着固定的套路，特征工程的建立凭借的更多的是经验，因此没有统一的方法。这里只是对一些常用的方法做一个总结。本文关注于特征选择部分。后面还有两篇会关注于特征表达和特征预处理。 

## 1、特征的来源

在进行数据分析的过程中，我们通常可以将数据分为两类：

> 数据中已经提供，我们需要从这些数据中推导出针对于问题的具有统计意义的高级特征。

> 数据中已经整理好了各类特征，我们需要从这些特征中筛选出针对于问题的有用的特征就ok了。

## 2、选择合适的特征

​	当我们的数据集中已经整理好各类特征了，这些数据可能有着成百上千的特征，那么我们该如何选拔出针对于问题有用的特征。

​	第一步，我们可以上网百度，或者去询问相关问题有关的专家，比如我们要处理一个药品疗效分类问题，那么我们可以去询问专家给出一些建议，询问哪些特征与这个分类问题有关，不管是这个特征对于分类效果的大或者小，我们都要记录下来。这是我们获取特征的第一个特征集。

​	因为这个特征集是一个比较大的特征集，我们需要进一步的筛选，这个特征集合有时候也可能很大，在尝试降维之前，我们有必要用特征工程的方法去选择出较重要的特征结合，这些方法不会用到领域知识，而仅仅是统计学的一些方法。 

​	我们可以尝试选择方差法进行筛选。相比较来说，特征的方差越大，这个特征对于我们来说就越有用，如果是方差等于1，那么这个特征对于我们来说，可以说是没有那么大的用处了，最差的情况，当方差等于0的时候，该特征与数据中的任何一个样本都没有关系，很显然，我们不需要这个特征。在实际应用中，我们会指定一个方差的阈值，当方差小于这个阈值的特征会被我们筛掉。sklearn中的VarianceThreshold类可以很方便的完成这个工作。 

特征选择方法一般分为三类：

> 第一类过滤法比较简单，它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。上面我们提到的方差筛选就是过滤法的一种。

> 第二类是包装法，根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。

> 第三类嵌入法则稍微复杂一点，它先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。类似于过滤法，但是它是通过机器学习训练来确定特征的优劣，而不是直接从特征的一些统计学指标来确定特征的优劣。

### 2.1、过滤法选择特征

​	上述我已经简述了过滤法选择特征的基本过程，下面我将介绍几种统计学指标来进行筛选出更具有统计意义的特征。

    1. 方差筛选法原理如上。

    2. 第二种也比较简单，采用的是相关系数法，这个主要应用于连续值的监督算法当中，我们可以计算出每一个特征与数据输出值之间的相关系数，设定一个阈值，筛选出相关系数较大的特征作为新的特征集。

    3. 第三种方法是利用假设检验法进行筛选出特征集，比如常用的有卡方检验，我下次单独更一篇文章，专门介绍卡方检验，卡方检验在一定程度上可以反映出特征分布和输出值的分布之间的联系（相关性），在sklearn中，可以使用chi2这个类来做卡方检验，可以得到所有特征的卡方值与显著性水平P临界值，我们可以给定卡方值阈值， 选择卡方值较大的部分特征即可。 

       > 不得不提起其他的假设验证的方法，我们还可以利用F检验和T检验，和卡方分布一样用的都是假设检验，同样都是根据统计分布找出相关性较大的特征，在sklearn中，有F检验的函数f_classif和f_regression ，通常用于回归问题和分类问题。

   	4. 信息增益的方法，我们可以从信息熵的角度进行分析特征和输出值之间的关系，这一点，在决策树算法中有所涉及，在信息论中有着详细的介绍，信息熵越大，说明出一个问题，该特征与输出值的相关性越高，就越需要保留，我们可以通过sklearn，通过使用mutual_info_classif(分类)和mutual_info_regression(回归) 进行计算出特征与输出值之间的信息熵。

我建议在进行特征选择的时候以上几种方法都可以进行试一试，这几种方法各有所长，综合使用吧。

### 2.2、包装法选择特征

​	包装法，我觉得还是比较厉害的，我们可以选中一个目标函数进行一步步的筛选出较好的特征集。

​	最经典的我觉得莫过于递归消除特征法（recursive feature elimination,简称RFE ），递归消除特征法的主要过程是，采用一个机器学习模型进行多次的训练，每一次的训练，都会消除若干部分权重系数的特征，然后再采用新的一组训练集进行训练。在sklearn中我们可以采用RFE函数进行筛选出我们所需要的特征。

> ​	以SVM支持向量机来做RFE的机器学习模型选择特征的例子最为经典。它在第一轮训练的时候，会选择所有的特征来训练，得到了分类的超平面$w \dot x+b=0$后，如果有n个特征，那么RFE-SVM会选择出$w$中分量的平方值$w_i^2$最小的那个序号i对应的特征，将其排除，在第二类的时候，特征数就剩下n-1个了，我们继续用这n-1个特征和输出值来训练SVM，同样的，去掉$w_i^2$最小的那个序号i对应的特征。以此类推，直到剩下的特征数满足我们的需求为止。 

### 2.3、嵌入式法选择特征

​	嵌入式法与包装法都是使用机器学习模型去选择特征，不同的是，嵌入式法是对全局的特征进行训练，而包装法是对训练集中的特征进行多轮次的训练模型，筛选出针对问题的特征集。

​	在sklearn中，我们可以采用SelectFromModel函数来选择特征 。

​	常用的L1正则化和L2正则化进行选择特征，正则化惩罚项越大，那么模型的系数就会越小。当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0. 但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择特征系数较大的特征。常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。 

​	一般来说，可以得到特征系数coef或者可以得到特征重要度(feature importances)的算法才可以做为嵌入法的基学习器 ，比如GBTD或者决策树。

## 3、寻找高级特征

​	当数据分析师已经整理好相对较好的特征集的时候，我们其实可以采用一些骚操作进行筛选出（不如说是自己组合创造出的特征）用网上对于高级特征的理解可以是：

> ​	比如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这个四级特征 。

​	在kaggle竞赛中，几乎每个强队都使用了集成算法，其中xgboot（“竞赛之王”之称）和LightBGM最为常见，一个好的特征工程，决定了一个模型的上限，寻找高级特征是优化最终模型的必由之路。

​	经验表明，聚类的时候高级特征尽量少一点，分类和回归问题的时候高级特征可以多一点。 

## 4、最后总结

​	在进行特征工程建立的过程中，尽量不能错过任何一个有用的特征，但是也不能使用过多的特征。

## 参考文献

参考论文《An introduction to variable and feature selection》

参考网页www.zhihu.com































​	

​	



